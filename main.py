#
# # main.py
# import asyncio
# import time
# import sqlite3
# import os
# from pathlib import Path
# from urllib.parse import urlencode
#
# from dotenv import load_dotenv
# from playwright.async_api import async_playwright
#
# from utils import notify_user
# from sites.seek_adapter import extract_seek_jobs
# from outputs import append_new_jobs_csv, build_html_from_db
#
#
# # ========= ç¯å¢ƒä¸é…ç½® =========
# load_dotenv()
#
# DB_PATH = "db.sqlite3"
# CHECK_INTERVAL = 600  # æ¯ 10 åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡ï¼ˆç§’ï¼‰
# USER_DATA_DIR = Path(__file__).parent / "user_data"
#
# def build_seek_url() -> str:
#     """ä» .env è¯»å–å…³é”®è¯/åœ°åŒºç”Ÿæˆæœç´¢ URL"""
#     keywords = os.getenv(
#         "SEEK_KEYWORDS",
#         "graduate OR tester OR QA OR developer OR marketing OR sales OR junior"
#     )
#     where = os.getenv("SEEK_LOCATION", "")  # ä¸ºç©ºåˆ™ä¸åŠ  where å‚æ•°
#     qs = {"keywords": keywords}
#     if where:
#         qs["where"] = where
#     return f"https://www.seek.co.nz/jobs?{urlencode(qs)}"
#
# SEARCH_URL = build_seek_url()
#
#
# # ========= æŒä¹…åŒ–æµè§ˆå™¨ä¸Šä¸‹æ–‡ =========
# def ensure_user_data_dir() -> bool:
#     """ç¡®ä¿ user_data æ˜¯ç›®å½•ï¼›è¿”å›æ˜¯å¦é¦–æ¬¡è¿è¡Œï¼ˆç›®å½•ä¸ºç©ºï¼‰"""
#     if USER_DATA_DIR.exists() and not USER_DATA_DIR.is_dir():
#         raise RuntimeError(
#             f"USER_DATA_DIR exists but is not a directory: {USER_DATA_DIR}\n"
#             f"Please delete/rename it, e.g. mv '{USER_DATA_DIR}' '{USER_DATA_DIR}.bak'"
#         )
#     USER_DATA_DIR.mkdir(parents=True, exist_ok=True)
#     return not any(USER_DATA_DIR.iterdir())  # ç›®å½•ä¸ºç©º => é¦–æ¬¡è¿è¡Œ
#
# async def get_persistent_context(playwright):
#     first_run = ensure_user_data_dir()
#     context = await playwright.chromium.launch_persistent_context(
#         user_data_dir=str(USER_DATA_DIR),
#         headless=False if first_run else True,  # é¦–æ¬¡å¯è§ï¼Œä¹‹åæ— å¤´
#         viewport={"width": 1280, "height": 800},
#         user_agent=(
#             "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
#             "AppleWebKit/537.36 (KHTML, like Gecko) "
#             "Chrome/122.0 Safari/537.36"
#         ),
#         slow_mo=50 if first_run else 0,
#     )
#     await context.set_extra_http_headers({
#         "Accept-Language": "en-NZ,en;q=0.9",
#         "Referer": "https://www.google.com/",
#     })
#
#     page = context.pages[0] if context.pages else await context.new_page()
#
#     if first_run:
#         print(">>> First run detected. A browser window opened.")
#         print(">>> Complete the Cloudflare check on Seek, wait for job results,")
#         print(">>> then return here and press ENTER.")
#         await page.goto(SEARCH_URL, wait_until="domcontentloaded")
#         input(">>> Press ENTER to continue after results are visible...")
#     return context
#
#
# # ========= æ•°æ®åº“ =========
# def init_db():
#     """åˆå§‹åŒ– SQLite æ•°æ®åº“ï¼ˆå¦‚å·²å­˜åœ¨åˆ™å¿½ç•¥ï¼‰"""
#     conn = sqlite3.connect(DB_PATH)
#     conn.execute("""
#         CREATE TABLE IF NOT EXISTS jobs (
#             job_id TEXT PRIMARY KEY,
#             title  TEXT,
#             link   TEXT,
#             seen_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
#         )
#     """)
#     conn.commit()
#     conn.close()
#
# def upsert_and_get_new(conn: sqlite3.Connection, jobs: list[dict]) -> list[dict]:
#     """å†™å…¥æœªè§è¿‡çš„èŒä½ï¼Œè¿”å›æœ¬è½®æ–°å¢çš„èŒä½åˆ—è¡¨"""
#     new_jobs = []
#     cur = conn.cursor()
#     for j in jobs:
#         job_id = j.get("job_id")
#         if not job_id:
#             continue
#         cur.execute("SELECT 1 FROM jobs WHERE job_id = ?", (job_id,))
#         if cur.fetchone():
#             continue
#         cur.execute(
#             "INSERT INTO jobs(job_id, title, link) VALUES(?,?,?)",
#             (job_id, j.get("title", ""), j.get("link", ""))
#         )
#         new_jobs.append(j)
#     conn.commit()
#     return new_jobs
#
#
# # ========= æ ¸å¿ƒç›‘æ§æµç¨‹ =========
# async def monitor_seek_jobs(playwright):
#     context = await get_persistent_context(playwright)
#     page = context.pages[0] if context.pages else await context.new_page()
#
#     # å¯ä¿¡ä¼šè¯ä¸‹è®¿é—®æœç´¢é¡µ
#     await page.goto(SEARCH_URL, wait_until="domcontentloaded")
#
#     # è§¦å‘æ‡’åŠ è½½ï¼ˆæ»šåŠ¨å‡ æ¬¡ï¼‰
#     for _ in range(6):
#         await page.mouse.wheel(0, 1500)
#         await page.wait_for_timeout(800)
#
#     # è¿™é‡ŒåŠ å…¥â€œå¥å£®æ€§ä¸å¯è§‚æµ‹æ€§â€çš„ try/except
#     try:
#         jobs = await extract_seek_jobs(page)
#     except Exception as e:
#         html = await page.content()
#         Path("debug_seek.html").write_text(html, encoding="utf-8")
#         await page.screenshot(path="debug_seek.png", full_page=True)
#         print("Extractor error:", e)
#         # å¯é€‰ï¼šé€šçŸ¥ä¸€æ¬¡
#         # notify_user(f"â—æŠ“å–å¤±è´¥ï¼š{e}")
#         await context.close()
#         return
#
#     # 1) æŠ“å–èŒä½
#     jobs = await extract_seek_jobs(page)
#
#     # 2) å»é‡å…¥åº“ï¼Œè·å¾—â€œæ–°èŒä½â€
#     conn = sqlite3.connect(DB_PATH)
#     new_jobs = upsert_and_get_new(conn, jobs)
#     conn.close()
#
#     # 3) é€šçŸ¥
#     if new_jobs:
#         print(f"æœ¬è½®æŠ“å– {len(jobs)} æ¡ï¼Œæ–°å¢ {len(new_jobs)} æ¡ã€‚")
#         for job in new_jobs:
#             notify_user(f"ğŸ†• {job['title']} - {job['company']} ({job['location']})\nğŸ”— {job['link']}")
#         # (b) è¿½åŠ å†™ CSVï¼ˆoutputs/new_jobs.csvï¼‰
#         append_new_jobs_csv(new_jobs)
#
#         # (c) ç”Ÿæˆ HTMLï¼ˆoutputs/latest.htmlï¼‰
#         html_path = build_html_from_db(DB_PATH, limit=100)
#         print(f"ğŸ“„ å·²ç”Ÿæˆ HTML åˆ—è¡¨ï¼š{html_path}")
#
#     else:
#         print("No new jobs this cycle.")
#
#     # æŒä¹…åŒ–ä¸Šä¸‹æ–‡ä¼šè‡ªåŠ¨ä¿å­˜ cookies
#     await context.close()
#
#
# # ========= å…¥å£ =========
# async def main():
#     init_db()
#     while True:
#         print(f"[{time.strftime('%H:%M:%S')}] å¼€å§‹æ£€æµ‹æ–°èŒä½...")
#         async with async_playwright() as pw:
#             await monitor_seek_jobs(pw)
#         print(f"ç­‰å¾… {CHECK_INTERVAL // 60} åˆ†é’Ÿåå†æ¬¡æ£€æŸ¥...\n")
#         time.sleep(CHECK_INTERVAL)
#
# if __name__ == "__main__":
#     asyncio.run(main())


# main.py
import os
import re
import time
import sqlite3
import asyncio
from pathlib import Path
from urllib.parse import urlencode

from dotenv import load_dotenv
from playwright.async_api import async_playwright

from utils import notify_user
from sites.seek_adapter import extract_seek_jobs
from outputs import append_new_jobs_csv, build_html_from_db


# ========= ç¯å¢ƒä¸é…ç½® =========
load_dotenv()

DB_PATH = "db.sqlite3"
CHECK_INTERVAL = int(os.getenv("CHECK_INTERVAL_SECONDS", "600"))  # é»˜è®¤ 10 åˆ†é’Ÿ
USER_DATA_DIR = Path(__file__).parent / "user_data"

def build_seek_url() -> str:
    """
    ä» .env è¯»å–å…³é”®è¯/åœ°åŒºç”Ÿæˆæœç´¢ URLã€‚
    æ”¯æŒ ORï¼Œä¾‹å¦‚ï¼šgraduate OR tester OR QA OR developer
    """
    keywords = os.getenv(
        "SEEK_KEYWORDS",
        "graduate OR tester OR QA OR developer OR junior"
    )
    where = os.getenv("SEEK_LOCATION", "").strip()  # ä¸ºç©ºåˆ™ä¸åŠ  where
    qs = {"keywords": keywords}
    if where:
        qs["where"] = where
    return f"https://www.seek.co.nz/jobs?{urlencode(qs)}"

SEARCH_URL = build_seek_url()


# ========= æŒä¹…åŒ–æµè§ˆå™¨ä¸Šä¸‹æ–‡ =========
def ensure_user_data_dir() -> bool:
    """ç¡®ä¿ user_data æ˜¯ç›®å½•ï¼›è¿”å›æ˜¯å¦é¦–æ¬¡è¿è¡Œï¼ˆç›®å½•ä¸ºç©ºï¼‰"""
    if USER_DATA_DIR.exists() and not USER_DATA_DIR.is_dir():
        raise RuntimeError(
            f"USER_DATA_DIR exists but is not a directory: {USER_DATA_DIR}\n"
            f"Please delete/rename it, e.g. mv '{USER_DATA_DIR}' '{USER_DATA_DIR}.bak'"
        )
    USER_DATA_DIR.mkdir(parents=True, exist_ok=True)
    return not any(USER_DATA_DIR.iterdir())  # ç›®å½•ä¸ºç©º => é¦–æ¬¡è¿è¡Œ

async def get_persistent_context(playwright):
    first_run = ensure_user_data_dir()
    context = await playwright.chromium.launch_persistent_context(
        user_data_dir=str(USER_DATA_DIR),
        headless=False if first_run else True,  # é¦–æ¬¡å¯è§ï¼Œä¹‹åæ— å¤´
        viewport={"width": 1280, "height": 800},
        user_agent=(
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/122.0 Safari/537.36"
        ),
        slow_mo=50 if first_run else 0,
    )
    await context.set_extra_http_headers({
        "Accept-Language": "en-NZ,en;q=0.9",
        "Referer": "https://www.google.com/",
    })

    page = context.pages[0] if context.pages else await context.new_page()

    if first_run:
        print(">>> First run detected. A browser window opened.")
        print(">>> Complete the Cloudflare check on Seek, wait for job results,")
        print(">>> then return here and press ENTER.")
        await page.goto(SEARCH_URL, wait_until="domcontentloaded")
        input(">>> Press ENTER to continue after results are visible...")
    return context


# ========= æ•°æ®åº“ï¼šå«è‡ªåŠ¨è¿ç§» company/location =========
def init_db():
    """åˆå§‹åŒ–æˆ–è¿ç§» SQLiteï¼šç¡®ä¿æœ‰ company/location ä¸¤åˆ—"""
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    # åˆ›å»ºï¼ˆè‹¥ä¸å­˜åœ¨ï¼‰
    cur.execute("""
        CREATE TABLE IF NOT EXISTS jobs (
            job_id   TEXT PRIMARY KEY,
            title    TEXT,
            link     TEXT,
            company  TEXT,
            location TEXT,
            seen_at  TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    # è¿ç§»è€è¡¨ç»“æ„
    cur.execute("PRAGMA table_info(jobs)")
    cols = {row[1] for row in cur.fetchall()}
    if "company" not in cols:
        cur.execute("ALTER TABLE jobs ADD COLUMN company TEXT")
    if "location" not in cols:
        cur.execute("ALTER TABLE jobs ADD COLUMN location TEXT")
    conn.commit()
    conn.close()

def upsert_and_get_new(conn: sqlite3.Connection, jobs: list[dict]) -> list[dict]:
    """å†™å…¥æœªè§è¿‡çš„èŒä½ï¼Œè¿”å›æœ¬è½®æ–°å¢çš„èŒä½åˆ—è¡¨"""
    new_jobs = []
    cur = conn.cursor()
    for j in jobs:
        job_id = j.get("job_id")
        if not job_id:
            continue
        cur.execute("SELECT 1 FROM jobs WHERE job_id = ?", (job_id,))
        if cur.fetchone():
            continue
        cur.execute(
            "INSERT INTO jobs(job_id, title, link, company, location) VALUES(?,?,?,?,?)",
            (job_id, j.get("title",""), j.get("link",""), j.get("company",""), j.get("location",""))
        )
        new_jobs.append(j)
    conn.commit()
    return new_jobs


# ========= è¿‡æ»¤ï¼šå‰”é™¤èµ„æ·±/ç®¡ç†å²— =========
_EXCLUDE_PAT = re.compile(
    r"\b(senior|intermediate|lead|leader|principal|head of|architect|director|manager)\b",
    re.IGNORECASE
)
def should_keep(job: dict) -> bool:
    title = (job.get("title") or "").lower()
    return not _EXCLUDE_PAT.search(title)


# ========= æ ¸å¿ƒç›‘æ§æµç¨‹ =========
async def monitor_seek_jobs(playwright):
    context = await get_persistent_context(playwright)
    page = context.pages[0] if context.pages else await context.new_page()

    try:
        # æœç´¢é¡µ
        await page.goto(SEARCH_URL, wait_until="domcontentloaded")

        # æ‡’åŠ è½½æ»šåŠ¨
        for _ in range(6):
            await page.mouse.wheel(0, 1500)
            await page.wait_for_timeout(800)

        # æŠ“å–
        jobs = await extract_seek_jobs(page)

        # è¿‡æ»¤æ‰ Senior/Intermediate/Leadâ€¦
        jobs = [j for j in jobs if should_keep(j)]

        # å…¥åº“å»é‡
        conn = sqlite3.connect(DB_PATH)
        new_jobs = upsert_and_get_new(conn, jobs)
        conn.close()

        # é€šçŸ¥ + è½åœ°
        if new_jobs:
            print(f"æœ¬è½®æŠ“å– {len(jobs)} æ¡ï¼Œæ–°å¢ {len(new_jobs)} æ¡ã€‚")
            for job in new_jobs:
                notify_user(f"ğŸ†• {job['title']} â€” {job['company']} Â· {job['location']}\nğŸ”— {job['link']}")
            # è¿½åŠ  CSV
            append_new_jobs_csv(new_jobs)
            # ç”Ÿæˆ HTML
            html_path = build_html_from_db(DB_PATH, limit=100)
            print(f"ğŸ“„ å·²ç”Ÿæˆ HTML åˆ—è¡¨ï¼š{html_path}")
        else:
            print("No new jobs this cycle.")

    except Exception as e:
        # æ–¹ä¾¿æ’æŸ¥
        try:
            Path("debug_seek.html").write_text(await page.content(), encoding="utf-8")
            await page.screenshot(path="debug_seek.png", full_page=True)
        except Exception:
            pass
        print("â— Extractor/monitor error:", e)
    finally:
        # å¯é€‰ï¼šä¿å­˜ä¼šè¯çŠ¶æ€ï¼ˆä¾¿äºå…¶å®ƒè„šæœ¬å¤ç”¨ï¼‰
        try:
            await context.storage_state(path="storage_state.json")
        except Exception:
            pass
        await context.close()


# ========= å…¥å£ =========
async def main():
    init_db()
    while True:
        print(f"[{time.strftime('%H:%M:%S')}] å¼€å§‹æ£€æµ‹æ–°èŒä½...")
        async with async_playwright() as pw:
            await monitor_seek_jobs(pw)
        print(f"ç­‰å¾… {CHECK_INTERVAL // 60} åˆ†é’Ÿåå†æ¬¡æ£€æŸ¥...\n")
        # ç”¨å¼‚æ­¥ sleepï¼Œé¿å…é˜»å¡
        await asyncio.sleep(CHECK_INTERVAL)

if __name__ == "__main__":
    asyncio.run(main())
