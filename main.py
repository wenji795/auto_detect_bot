
# main.py
import os
import re
import time
import sqlite3
import asyncio
from pathlib import Path
from urllib.parse import urlencode

from dotenv import load_dotenv
from playwright.async_api import async_playwright, BrowserContext

from utils import notify_user
from sites.seek_adapter import extract_seek_jobs
from sites.linkedin_adapter import extract_linkedin_jobs
from outputs import append_new_jobs_csv, build_html_from_db


# ========= ç¯å¢ƒä¸é…ç½® =========
load_dotenv()
HEADFUL_FLAG = os.getenv("HEADFUL", "0") == "1"
LOGIN_TARGET = os.getenv("LOGIN_TARGET", "seek").lower()
DB_PATH = "db.sqlite3"
CHECK_INTERVAL = int(os.getenv("CHECK_INTERVAL_SECONDS", "600"))  # é»˜è®¤ 10 åˆ†é’Ÿ
USER_DATA_DIR = Path(__file__).parent / "user_data"

def build_seek_url() -> str:
    """
    ä» .env è¯»å–å…³é”®è¯/åœ°åŒº/åˆ†ç±»ç”Ÿæˆ SEEK æœç´¢ URLã€‚
    æ”¯æŒ ORï¼Œä¾‹å¦‚ï¼šgraduate OR tester OR QA OR developer
    """
    keywords = os.getenv(
        "SEEK_KEYWORDS",
        "graduate OR tester OR QA OR developer OR junior"
    ).strip()
    where = os.getenv("SEEK_LOCATION", "").strip()
    classification = os.getenv("SEEK_CLASSIFICATION", "").strip()   # e.g. information-communication-technology
    subclass = os.getenv("SEEK_SUBCLASS", "").strip()               # e.g. testing-quality-assurance,developers-programmers

    qs = {"keywords": keywords}
    if where:
        qs["where"] = where
    if classification:
        qs["classification"] = classification
    if subclass:
        qs["subclassification"] = subclass
    return f"https://www.seek.co.nz/jobs?{urlencode(qs)}"

def build_linkedin_url() -> str:
    """
    ç”Ÿæˆ LinkedIn Jobs æœç´¢ URLï¼ˆç™»å½•åæ›´ç¨³å®šï¼‰ã€‚
    f_TPR: r86400=24h, r604800=7å¤©, r2592000=30å¤©
    """
    keywords = os.getenv(
        "LINKEDIN_KEYWORDS",
        "graduate OR junior OR qa OR tester OR automation OR developer OR software"
    ).strip().replace(" ", "%20")
    location = os.getenv("LINKEDIN_LOCATION", "New Zealand").strip().replace(" ", "%20")
    time_range = os.getenv("LINKEDIN_TIME_RANGE", "r604800")  # æœ€è¿‘ 7 å¤©

    return (
        "https://www.linkedin.com/jobs/search/"
        f"?keywords={keywords}&location={location}&f_TPR={time_range}"
    )

SEEK_URL = build_seek_url()
LINKEDIN_URL = build_linkedin_url()


# ========= æŒä¹…åŒ–æµè§ˆå™¨ä¸Šä¸‹æ–‡ =========
def ensure_user_data_dir() -> bool:
    """ç¡®ä¿ user_data æ˜¯ç›®å½•ï¼›è¿”å›æ˜¯å¦é¦–æ¬¡è¿è¡Œï¼ˆç›®å½•ä¸ºç©ºï¼‰"""
    if USER_DATA_DIR.exists() and not USER_DATA_DIR.is_dir():
        raise RuntimeError(
            f"USER_DATA_DIR exists but is not a directory: {USER_DATA_DIR}\n"
            f"Please delete/rename it, e.g. mv '{USER_DATA_DIR}' '{USER_DATA_DIR}.bak'"
        )
    USER_DATA_DIR.mkdir(parents=True, exist_ok=True)
    return not any(USER_DATA_DIR.iterdir())  # ç›®å½•ä¸ºç©º => é¦–æ¬¡è¿è¡Œ

async def get_persistent_context(pw) -> BrowserContext:
    first_run = ensure_user_data_dir()
    headless = False if (first_run or HEADFUL_FLAG) else True

    context = await pw.chromium.launch_persistent_context(
        user_data_dir=str(USER_DATA_DIR),
        headless=headless,
        viewport={"width": 1280, "height": 800},
        user_agent=(
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/122.0 Safari/537.36"
        ),
        slow_mo=50 if not headless else 0,
    )
    await context.set_extra_http_headers({
        "Accept-Language": "en-NZ,en;q=0.9",
        "Referer": "https://www.google.com/",
    })

    page = context.pages[0] if context.pages else await context.new_page()

    # åªè¦æ˜¯é¦–æ¬¡æˆ–ä½ æ˜¾å¼è¦æ±‚å¼€çª—ï¼Œå°±å¼•å¯¼ç™»å½•
    if first_run or HEADFUL_FLAG:
        mode = LOGIN_TARGET  # 'seek' / 'linkedin' / 'both'
        print(f">>> Interactive session. HEADFUL={not headless}, LOGIN_TARGET={mode}")

        if mode == "linkedin":
            await page.goto(LINKEDIN_URL, wait_until="domcontentloaded")
        elif mode == "both":
            # å…ˆå¼€ Seek
            await page.goto(SEEK_URL, wait_until="domcontentloaded")
            # å†å¼€ä¸€ä¸ªæ ‡ç­¾åˆ° LinkedInï¼Œå¹¶åˆ‡è¿‡å»
            ln = await context.new_page()
            await ln.goto(LINKEDIN_URL, wait_until="domcontentloaded")
            try:
                await ln.wait_for_load_state("networkidle", timeout=8000)
            except Exception:
                pass
            page = ln  # èšç„¦åœ¨ LinkedIn æ ‡ç­¾
        else:  # é»˜è®¤ seek
            await page.goto(SEEK_URL, wait_until="domcontentloaded")

        try:
            await page.wait_for_load_state("networkidle", timeout=8000)
        except Exception:
            pass
        input(">>> åœ¨æ‰“å¼€çš„æµè§ˆå™¨é‡Œå®Œæˆç™»å½•/éªŒè¯åï¼Œå›åˆ°è¿™é‡ŒæŒ‰ ENTER ç»§ç»­â€¦")
    return context


# ========= æ•°æ®åº“ï¼ˆå« source åˆ— & å”¯ä¸€ç´¢å¼•ï¼‰ =========
def init_db():
    """
    åˆå§‹åŒ–æˆ–è¿ç§» SQLiteï¼š
      - å¢åŠ  company/location/source åˆ—
      - å»ºå”¯ä¸€ç´¢å¼• (source, job_id) é¿å…å¹³å°é—´å†²çª
    """
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute("""
        CREATE TABLE IF NOT EXISTS jobs (
            job_id   TEXT,
            title    TEXT,
            link     TEXT,
            company  TEXT,
            location TEXT,
            source   TEXT DEFAULT 'seek',
            seen_at  TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    # è¿ç§»ï¼šè¡¥åˆ—
    cur.execute("PRAGMA table_info(jobs)")
    cols = {row[1] for row in cur.fetchall()}
    if "company" not in cols:
        cur.execute("ALTER TABLE jobs ADD COLUMN company TEXT")
    if "location" not in cols:
        cur.execute("ALTER TABLE jobs ADD COLUMN location TEXT")
    if "source" not in cols:
        cur.execute("ALTER TABLE jobs ADD COLUMN source TEXT DEFAULT 'seek'")
        cur.execute("UPDATE jobs SET source='seek' WHERE source IS NULL OR source=''")
    # å”¯ä¸€ç´¢å¼•ï¼ˆè‹¥ä¸å­˜åœ¨ï¼‰
    cur.execute("""
        CREATE UNIQUE INDEX IF NOT EXISTS idx_jobs_source_jobid
        ON jobs(source, job_id)
    """)
    conn.commit()
    conn.close()

async def safe_dump_page(page, html_path: str, png_path: str):
    # å°è¯•ç­‰å¾…ä¸€ä¸ªç¨³å®šçŠ¶æ€ï¼›å¦‚æœåœ¨å¯¼èˆªï¼Œä¼šæŠ›å¼‚å¸¸ï¼Œå¿½ç•¥å³å¯
    for state in ("networkidle", "domcontentloaded", "load"):
        try:
            await page.wait_for_load_state(state, timeout=3000)
            break
        except Exception:
            pass
    # å°è¯•ç”¨ evaluate æ‹¿æ•´é¡µ HTMLï¼ˆæ¯” page.content() æ›´ä¸å®¹æ˜“æ’å¯¼èˆªé”™è¯¯ï¼‰
    try:
        html = await page.evaluate("() => document.documentElement.outerHTML")
        Path(html_path).write_text(html or "", encoding="utf-8")
    except Exception:
        pass
    # æˆªå›¾ä¹Ÿåšä¿æŠ¤
    try:
        await page.screenshot(path=png_path, full_page=True)
    except Exception:
        pass

async def interactive_login_if_needed(pw, context: BrowserContext, target_url: str, label: str = "LinkedIn"):
    """
    å¦‚æœæ£€æµ‹åˆ°ç½‘ç«™éœ€è¦ç™»å½•ï¼š
    - å¯åŠ¨ä¸€ä¸ªä½¿ç”¨â€œä¸´æ—¶ user_data ç›®å½•â€çš„å¯è§æµè§ˆå™¨è®©ä½ ç™»å½•ï¼›
    - å¯¼å‡º cookies/localStorageï¼›
    - æ³¨å…¥åˆ°å½“å‰çš„ headless æŒä¹…åŒ– contextï¼›
    - å…³é—­ä¸´æ—¶æµè§ˆå™¨ï¼Œç»§ç»­æ— å¤´æŠ“å–ã€‚
    """
    page = await context.new_page()
    await page.goto(target_url, wait_until="domcontentloaded")
    try:
        await page.wait_for_load_state("networkidle", timeout=5000)
    except Exception:
        pass

    url_lc = (page.url or "").lower()
    try:
        html_text = await page.evaluate("() => document.body ? document.body.innerText : ''")
        html_lc = (html_text or "").lower()
    except Exception:
        html_lc = ""

    # æ¢æµ‹ç™»å½•è¡¨å•ï¼ˆLinkedInï¼š#username æˆ– name=session_keyï¼‰
    login_form_present = False
    try:
        login_form_present = (await page.locator('input#username, input[name="session_key"]').count()) > 0
    except Exception:
        pass

    await page.close()

    def needs_login() -> bool:
        return (
            "linkedin.com/login" in url_lc
            or "checkpoint" in url_lc
            or login_form_present
            or ("sign in" in html_lc)
        )

    if not needs_login():
        return context

    print(f"ğŸ” æ£€æµ‹åˆ° {label} éœ€è¦ç™»å½•ï¼Œå¼€å¯ä¸´æ—¶å¯è§æµè§ˆå™¨è¿›è¡Œç™»å½•ï¼ˆä¸å…³é—­å½“å‰ä¸Šä¸‹æ–‡ï¼‰ã€‚")

    # 1) ç”¨â€œä¸´æ—¶ user_data ç›®å½•â€å¯ä¸€ä¸ª headful æŒä¹…åŒ–æµè§ˆå™¨ï¼Œé¿å…ä¸ä¸» profile å†²çª
    tmp_dir = USER_DATA_DIR.parent / "user_data_login_tmp"
    tmp_dir.mkdir(parents=True, exist_ok=True)

    headful_ctx = await pw.chromium.launch_persistent_context(
        user_data_dir=str(tmp_dir),
        headless=False,
        viewport={"width": 1280, "height": 800},
        user_agent=(
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/122.0 Safari/537.36"
        ),
        slow_mo=50,
    )
    await headful_ctx.set_extra_http_headers({
        "Accept-Language": "en-NZ,en;q=0.9",
        "Referer": "https://www.google.com/",
    })

    hp = headful_ctx.pages[0] if headful_ctx.pages else await headful_ctx.new_page()
    await hp.goto(target_url, wait_until="domcontentloaded")
    try:
        await hp.wait_for_load_state("networkidle", timeout=8000)
    except Exception:
        pass

    print(f">>> å·²æ‰“å¼€ {label} ç™»å½•é¡µï¼Œè¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•ï¼Œç„¶åå›åˆ°ç»ˆç«¯æŒ‰ ENTER ç»§ç»­ã€‚")
    input(">>> ç™»å½•å®ŒæˆåæŒ‰ ENTER ç»§ç»­â€¦")

    # 2) å¯¼å‡º storage_state
    storage = None
    try:
        storage = await headful_ctx.storage_state()
    except Exception:
        storage = None

    # 3) æ³¨å…¥åˆ°å½“å‰ headless æŒä¹…åŒ– context
    if storage:
        # 3a) cookies
        try:
            cookies = storage.get("cookies", [])
            if cookies:
                await context.add_cookies(cookies)
        except Exception:
            pass

        # 3b) localStorageï¼ˆæŒ‰ origin å†™å…¥ï¼‰
        try:
            origins = storage.get("origins", [])
            for origin in origins:
                origin_url = origin.get("origin")
                ls_items = origin.get("localStorage", [])
                if not origin_url or not ls_items:
                    continue

                p = await context.new_page()
                # æ‰“å¼€ originï¼Œå†™å…¥ localStorage
                await p.goto(origin_url, wait_until="domcontentloaded")
                for kv in ls_items:
                    key = kv.get("name", "")
                    val = kv.get("value", "")
                    if key:
                        await p.evaluate(
                            """([k, v]) => { try { localStorage.setItem(k, v); } catch(e){} }""",
                            [key, val],
                        )
                await p.close()
        except Exception:
            pass

    # å…³é—­ä¸´æ—¶ headful æµè§ˆå™¨
    try:
        await headful_ctx.close()
    except Exception:
        pass

    print(f"âœ… {label} ç™»å½•æ€å·²æ³¨å…¥ï¼Œæ— å¤´æ¨¡å¼ç»§ç»­ã€‚")
    return context


def upsert_and_get_new(conn: sqlite3.Connection, jobs: list[dict], source: str) -> list[dict]:
    """å†™å…¥æœªè§è¿‡çš„èŒä½ï¼ˆæŒ‰ source+job_id å»é‡ï¼‰ï¼Œè¿”å›æœ¬è½®æ–°å¢"""
    new_jobs = []
    cur = conn.cursor()
    for j in jobs:
        job_id = j.get("job_id")
        if not job_id:
            continue
        # æŸ¥é‡
        cur.execute("SELECT 1 FROM jobs WHERE source=? AND job_id=?", (source, job_id))
        if cur.fetchone():
            continue
        # æ’å…¥
        cur.execute(
            "INSERT INTO jobs(job_id, title, link, company, location, source) VALUES(?,?,?,?,?,?)",
            (job_id, j.get("title",""), j.get("link",""), j.get("company",""), j.get("location",""), source)
        )
        new_jobs.append(j)
    conn.commit()
    return new_jobs


# ========= è¿‡æ»¤ï¼šå‰”é™¤èµ„æ·±/ç®¡ç†å²—ï¼ˆå¯å åŠ ä½ çš„ IT è¿‡æ»¤ï¼‰ =========
_EXCLUDE_SENIORITY = re.compile(
    r"\b(senior|intermediate|lead|leader|principal|head of|architect|director|manager)\b",
    re.IGNORECASE
)
def should_keep(job: dict) -> bool:
    title = (job.get("title") or "").lower()
    return not _EXCLUDE_SENIORITY.search(title)


# ========= ç»Ÿä¸€çš„å†™åº“/é€šçŸ¥/è½åœ° =========
def finalize_batch(source: str, grabbed: list[dict]):
    # è¿‡æ»¤
    jobs = [j for j in grabbed if should_keep(j)]
    # å…¥åº“
    conn = sqlite3.connect(DB_PATH)
    new_jobs = upsert_and_get_new(conn, jobs, source=source)
    conn.close()
    # é€šçŸ¥ & è¾“å‡º
    if new_jobs:
        print(f"[{source}] æŠ“å– {len(jobs)} æ¡ï¼Œæ–°å¢ {len(new_jobs)} æ¡ã€‚")
        for job in new_jobs:
            notify_user(f"ğŸ†• [{source.capitalize()}] {job['title']} â€” {job['company']} Â· {job['location']}\nğŸ”— {job['link']}")
        append_new_jobs_csv(new_jobs)
        html_path = build_html_from_db(DB_PATH, limit=100)
        print(f"ğŸ“„ å·²ç”Ÿæˆ HTML åˆ—è¡¨ï¼š{html_path}")
    else:
        print(f"[{source}] No new jobs this cycle.")


# ========= ä¸¤ä¸ªå¹³å°çš„ monitor =========
async def monitor_seek(context: BrowserContext):
    page = context.pages[0] if context.pages else await context.new_page()
    try:
        await page.goto(SEEK_URL, wait_until="domcontentloaded")
        for _ in range(6):
            await page.mouse.wheel(0, 1500)
            await page.wait_for_timeout(800)
        jobs = await extract_seek_jobs(page)
        finalize_batch("seek", jobs)
    except Exception as e:
        try:
            Path("debug_seek.html").write_text(await page.content(), encoding="utf-8")
            await page.screenshot(path="debug_seek.png", full_page=True)
        except Exception:
            pass
        print("â— SEEK monitor error:", e)

async def monitor_linkedin(context):
    page = await context.new_page()
    try:
        await page.goto(LINKEDIN_URL, wait_until="domcontentloaded")
        # å…œåº•ï¼šå¦‚æœè¢«é‡å®šå‘åˆ° feedï¼Œå¼ºåˆ¶å›åˆ°æœç´¢é¡µ
        if "linkedin.com/feed" in (page.url or "").lower():
            await page.goto(LINKEDIN_URL, wait_until="domcontentloaded")

        # å°è¯•æ¥å— cookie / åŒæ„æŒ‰é’®ï¼ˆå›½é™…åŒ–å…œåº•ï¼‰
        for sel in [
            'button:has-text("Accept")',
            'button:has-text("åŒæ„")',
            'button:has-text("Agree")',
            'button[aria-label*="Accept"]',
            'button[aria-label*="accept"]'
        ]:
            try:
                await page.locator(sel).first.click(timeout=2000)
                break
            except Exception:
                pass

        # ç­‰ä¸€ç­‰è®© SPA ç¨³å®š
        try:
            await page.wait_for_load_state("networkidle", timeout=8000)
        except Exception:
            pass

        # è½»å¾®æ»šåŠ¨è§¦å‘æ‡’åŠ è½½
        for _ in range(4):
            await page.mouse.wheel(0, 1200)
            await page.wait_for_timeout(600)

        jobs = await extract_linkedin_jobs(page)
        finalize_batch("linkedin", jobs)

    except Exception as e:
        await safe_dump_page(page, "debug_linkedin.html", "debug_linkedin.png")
        print("â— LinkedIn monitor error:", e)
    finally:
        await page.close()



# ========= å…¥å£ =========
async def main():
    init_db()
    while True:
        print(f"[{time.strftime('%H:%M:%S')}] å¼€å§‹æ£€æµ‹æ–°èŒä½...")
        async with async_playwright() as pw:
            context = await get_persistent_context(pw)
            # ä¸€è½®è·‘ä¸¤ä¸ªç«™ç‚¹
            await monitor_seek(context)
            context = await interactive_login_if_needed(pw, context, LINKEDIN_URL, label="LinkedIn")
            await monitor_linkedin(context)

            # ä¿å­˜ç™»å½•æ€ä»¥ä¾¿å…¶å®ƒè„šæœ¬å¤ç”¨
            try:
                await context.storage_state(path="storage_state.json")
            except Exception:
                pass
            await context.close()

        print(f"ç­‰å¾… {CHECK_INTERVAL // 60} åˆ†é’Ÿåå†æ¬¡æ£€æŸ¥...\n")
        await asyncio.sleep(CHECK_INTERVAL)


if __name__ == "__main__":
    asyncio.run(main())
