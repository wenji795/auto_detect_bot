#
# import asyncio#Python çš„å¼‚æ­¥æ¡†æ¶ï¼Œç”¨æ¥æ‰§è¡Œå¼‚æ­¥æ“ä½œï¼ˆPlaywright å°±æ˜¯å¼‚æ­¥çš„ï¼‰ã€‚
# import time
# import sqlite3#è½»é‡çº§æ•°æ®åº“ï¼Œç”¨æ¥ä¿å­˜å·²æ£€æµ‹è¿‡çš„èŒä½ã€‚
# from dotenv import load_dotenv#ä» .env æ–‡ä»¶åŠ è½½ç¯å¢ƒå˜é‡ï¼ˆ
# from pathlib import Path#å¤„ç†æ–‡ä»¶è·¯å¾„ï¼Œé¿å…æ‰‹å†™å­—ç¬¦ä¸²ã€‚
# from playwright.async_api import async_playwright#Playwright å¼‚æ­¥ç‰ˆ APIã€‚
# from utils import notify_user#è‡ªå®šä¹‰æ¨¡å—ï¼Œè´Ÿè´£å‘é€é€šçŸ¥ï¼ˆæ¯”å¦‚ Telegramï¼‰ã€‚
# from sites.seek_adapter import extract_seek_jobs
#
#
# SEARCH_URL = "https://www.seek.co.nz/jobs?keywords=graduate+developer"
# USER_DATA_DIR = Path(__file__).parent / "user_data"#ä¿å­˜æµè§ˆå™¨çš„ç”¨æˆ·æ•°æ®ï¼ˆcookiesã€ç¼“å­˜ç­‰ï¼‰ç›®å½•ï¼Œé˜²æ­¢æ¯æ¬¡å¯åŠ¨éƒ½è¢« Cloudflare éªŒè¯ã€‚
#
# def ensure_user_data_dir() -> bool:
#     """ç¡®ä¿ user_data æ˜¯ä¸ªç›®å½•ï¼›è¿”å›æ˜¯å¦é¦–æ¬¡è¿è¡Œï¼ˆç›®å½•ä¸ºç©ºï¼‰
#     æ£€æŸ¥ user_data æ˜¯å¦å­˜åœ¨ã€‚
#     å¦‚æœæ˜¯æ–‡ä»¶è€Œä¸æ˜¯æ–‡ä»¶å¤¹ â†’ æŠ›å‡ºå¼‚å¸¸æç¤ºä½ åˆ é™¤ã€‚è‹¥ä¸å­˜åœ¨åˆ™åˆ›å»ºã€‚
#     å¦‚æœç›®å½•æ˜¯ç©ºçš„ï¼Œè¯´æ˜æ˜¯ç¬¬ä¸€æ¬¡è¿è¡Œï¼ˆfirst_run=Trueï¼‰ï¼Œå¦åˆ™è¡¨ç¤ºå·²ç»ä¿å­˜äº† session cookieã€‚"""
#     if USER_DATA_DIR.exists() and not USER_DATA_DIR.is_dir():
#         raise RuntimeError(
#             f"USER_DATA_DIR exists but is not a directory: {USER_DATA_DIR}\n"
#             f"Please delete/rename it, e.g. mv '{USER_DATA_DIR}' '{USER_DATA_DIR}.bak'"
#         )
#     USER_DATA_DIR.mkdir(parents=True, exist_ok=True)
#     # ç›®å½•ä¸ºç©º => é¦–æ¬¡è¿è¡Œ
#     return not any(USER_DATA_DIR.iterdir())
#
# async def get_persistent_context(playwright):
#     """launch_persistent_context() åˆ›å»ºæŒä¹…åŒ–æµè§ˆå™¨ä¸Šä¸‹æ–‡ï¼Œèƒ½ä¿å­˜ç™»å½•ä¿¡æ¯ã€cookiesã€‚
#     headless=Falseï¼šé¦–æ¬¡è¿è¡Œæ—¶è®©ä½ çœ‹åˆ°æµè§ˆå™¨çª—å£ï¼ˆæ–¹ä¾¿æ‰‹åŠ¨é€šè¿‡ Cloudflare éªŒè¯ï¼‰ã€‚
#     headless=Trueï¼šä¹‹åè‡ªåŠ¨åå°è¿è¡Œã€‚
#     slow_mo=50ï¼šæ¨¡æ‹Ÿäººç±»æ“ä½œé€Ÿåº¦ã€‚
#     user_agentï¼šä¼ªè£…æˆçœŸå®ç”¨æˆ·æµè§ˆå™¨ã€‚"""
#     first_run = ensure_user_data_dir()
#
#     context = await playwright.chromium.launch_persistent_context(
#         user_data_dir=str(USER_DATA_DIR),
#         headless=False if first_run else True,  # é¦–æ¬¡å¯è§ï¼Œä¹‹åæ— å¤´
#         viewport={"width": 1280, "height": 800},
#         user_agent=(
#             "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
#             "AppleWebKit/537.36 (KHTML, like Gecko) "
#             "Chrome/122.0 Safari/537.36"
#         ),
#         slow_mo=50 if first_run else 0,
#     )
#
#     #å¢åŠ æ›´â€œäººç±»â€çš„ HTTP è¯·æ±‚å¤´ï¼Œé™ä½è¢«æ£€æµ‹ä¸ºçˆ¬è™«çš„æ¦‚ç‡ã€‚
#     await context.set_extra_http_headers({
#         "Accept-Language": "en-NZ,en;q=0.9",
#         "Referer": "https://www.google.com/",
#     })
#
#     """
#     è·å–ä¸€ä¸ªå¯ç”¨çš„ page å¯¹è±¡ã€‚
#     å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡è¿è¡Œï¼Œæ‰“å¼€æµè§ˆå™¨çª—å£ï¼š
#         è®©ä½ æ‰‹åŠ¨é€šè¿‡ Cloudflare éªŒè¯ï¼›
#         åŠ è½½èŒä½é¡µï¼›
#         ç­‰ä½ ç¡®è®¤é¡µé¢åŠ è½½å®ŒåæŒ‰ Enterã€‚
#     éªŒè¯å®Œæˆå cookies ä¼šè‡ªåŠ¨ä¿å­˜åˆ° user_data/ï¼Œä¸‹æ¬¡è¿è¡Œå°±ä¸å†éœ€è¦äººå·¥éªŒè¯ã€‚"""
#     page = context.pages[0] if context.pages else await context.new_page()
#     if first_run:
#         print(">>> First run detected. A browser window opened.")
#         print(">>> Complete the Cloudflare check on Seek, wait for job results,")
#         print(">>> then return here and press ENTER.")
#         await page.goto(SEARCH_URL, wait_until="domcontentloaded")
#         input(">>> Press ENTER to continue after results are visible...")
#     return context
#
# # åŠ è½½ç¯å¢ƒå˜é‡ï¼ˆ.envï¼‰
# load_dotenv()
#
# DB_PATH = "db.sqlite3"
# CHECK_INTERVAL = 600  # æ¯10åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡ï¼ˆå•ä½ï¼šç§’ï¼‰
#
# """æ‰“å¼€æˆ–åˆ›å»ºä¸€ä¸ª SQLite æ•°æ®åº“ db.sqlite3ï¼›
# å»ºè¡¨ï¼šä¿å­˜èŒä½ä¿¡æ¯ï¼ˆidã€titleã€linkã€æ—¶é—´ï¼‰ï¼›
# IF NOT EXISTS é¿å…é‡å¤åˆ›å»ºã€‚"""
# # ========= æ•°æ®åº“ =========
# def init_db():
#     """åˆå§‹åŒ– SQLite æ•°æ®åº“"""
#     conn = sqlite3.connect(DB_PATH)
#     conn.execute("""
#         CREATE TABLE IF NOT EXISTS jobs (
#             job_id TEXT PRIMARY KEY,
#             title TEXT,
#             link TEXT,
#             seen_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
#         )
#     """)
#     conn.commit()
#     conn.close()
#
#
# def upsert_and_get_new(conn: sqlite3.Connection, jobs: list[dict]) -> list[dict]:
#     """å†™å…¥æœªè§è¿‡çš„èŒä½ï¼Œè¿”å›æœ¬è½®æ–°å¢çš„èŒä½åˆ—è¡¨"""
#     new_jobs = []
#     cur = conn.cursor()
#     for j in jobs:
#         job_id = j.get("job_id")
#         if not job_id:
#             continue
#         cur.execute("SELECT 1 FROM jobs WHERE job_id = ?", (job_id,))
#         if cur.fetchone():
#             continue
#         cur.execute(
#             "INSERT INTO jobs(job_id, title, link) VALUES(?,?,?)",
#             (job_id, j.get("title", ""), j.get("link", ""))
#         )
#         new_jobs.append(j)
#     conn.commit()
#     return new_jobs
#
# # ========= æ ¸å¿ƒç›‘æ§æµç¨‹ =========
# async def monitor_seek_jobs(playwright):
#     context = await get_persistent_context(playwright)
#     page = context.pages[0] if context.pages else await context.new_page()
#
#     # å¯ä¿¡ä¼šè¯ä¸‹è®¿é—®æœç´¢é¡µ
#     await page.goto(SEARCH_URL, wait_until="domcontentloaded")
#
#     # è§¦å‘æ‡’åŠ è½½ï¼ˆæ»šåŠ¨å‡ æ¬¡ï¼‰
#     for _ in range(6):
#         await page.mouse.wheel(0, 1500)
#         await page.wait_for_timeout(800)
#
#     # 1) æŠ“å–èŒä½
#     jobs = await extract_seek_jobs(page)
#
#     # 2) å»é‡å…¥åº“ï¼Œè·å¾—â€œæ–°èŒä½â€
#     conn = sqlite3.connect(DB_PATH)
#     new_jobs = upsert_and_get_new(conn, jobs)
#     conn.close()
#
#     # 3) é€šçŸ¥
#     if new_jobs:
#         print(f"æœ¬è½®æŠ“å– {len(jobs)} æ¡ï¼Œæ–°å¢ {len(new_jobs)} æ¡ã€‚")
#         for job in new_jobs:
#             notify_user(f"ğŸ†• {job['title']} - {job['company']} ({job['location']})\nğŸ”— {job['link']}")
#     else:
#         print("No new jobs this cycle.")
#
#     # æŒä¹…åŒ–ä¸Šä¸‹æ–‡ä¼šè‡ªåŠ¨ä¿å­˜ cookies
#     await context.close()
#
#
# async def main():
#     """ä¸»å¾ªç¯ï¼šå®šæ—¶ç›‘æ§ Seek"""
#     init_db()
#
#     while True:
#         print(f"[{time.strftime('%H:%M:%S')}] å¼€å§‹æ£€æµ‹æ–°èŒä½...")
#         async with async_playwright() as playwright:
#             await monitor_seek_jobs(playwright)
#         print(f"ç­‰å¾… {CHECK_INTERVAL // 60} åˆ†é’Ÿåå†æ¬¡æ£€æŸ¥...\n")
#         time.sleep(CHECK_INTERVAL)
#
#
# if __name__ == "__main__":
#     asyncio.run(main())
# main.py
import asyncio
import time
import sqlite3
import os
from pathlib import Path
from urllib.parse import urlencode

from dotenv import load_dotenv
from playwright.async_api import async_playwright

from utils import notify_user
from sites.seek_adapter import extract_seek_jobs

# ========= ç¯å¢ƒä¸é…ç½® =========
load_dotenv()

DB_PATH = "db.sqlite3"
CHECK_INTERVAL = 600  # æ¯ 10 åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡ï¼ˆç§’ï¼‰
USER_DATA_DIR = Path(__file__).parent / "user_data"

def build_seek_url() -> str:
    """ä» .env è¯»å–å…³é”®è¯/åœ°åŒºç”Ÿæˆæœç´¢ URL"""
    keywords = os.getenv("SEEK_KEYWORDS", "graduate developer")
    where = os.getenv("SEEK_LOCATION", "")  # ä¸ºç©ºåˆ™ä¸åŠ  where å‚æ•°
    qs = {"keywords": keywords}
    if where:
        qs["where"] = where
    return f"https://www.seek.co.nz/jobs?{urlencode(qs)}"

SEARCH_URL = build_seek_url()


# ========= æŒä¹…åŒ–æµè§ˆå™¨ä¸Šä¸‹æ–‡ =========
def ensure_user_data_dir() -> bool:
    """ç¡®ä¿ user_data æ˜¯ç›®å½•ï¼›è¿”å›æ˜¯å¦é¦–æ¬¡è¿è¡Œï¼ˆç›®å½•ä¸ºç©ºï¼‰"""
    if USER_DATA_DIR.exists() and not USER_DATA_DIR.is_dir():
        raise RuntimeError(
            f"USER_DATA_DIR exists but is not a directory: {USER_DATA_DIR}\n"
            f"Please delete/rename it, e.g. mv '{USER_DATA_DIR}' '{USER_DATA_DIR}.bak'"
        )
    USER_DATA_DIR.mkdir(parents=True, exist_ok=True)
    return not any(USER_DATA_DIR.iterdir())  # ç›®å½•ä¸ºç©º => é¦–æ¬¡è¿è¡Œ

async def get_persistent_context(playwright):
    first_run = ensure_user_data_dir()
    context = await playwright.chromium.launch_persistent_context(
        user_data_dir=str(USER_DATA_DIR),
        headless=False if first_run else True,  # é¦–æ¬¡å¯è§ï¼Œä¹‹åæ— å¤´
        viewport={"width": 1280, "height": 800},
        user_agent=(
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/122.0 Safari/537.36"
        ),
        slow_mo=50 if first_run else 0,
    )
    await context.set_extra_http_headers({
        "Accept-Language": "en-NZ,en;q=0.9",
        "Referer": "https://www.google.com/",
    })

    page = context.pages[0] if context.pages else await context.new_page()

    if first_run:
        print(">>> First run detected. A browser window opened.")
        print(">>> Complete the Cloudflare check on Seek, wait for job results,")
        print(">>> then return here and press ENTER.")
        await page.goto(SEARCH_URL, wait_until="domcontentloaded")
        input(">>> Press ENTER to continue after results are visible...")
    return context


# ========= æ•°æ®åº“ =========
def init_db():
    """åˆå§‹åŒ– SQLite æ•°æ®åº“ï¼ˆå¦‚å·²å­˜åœ¨åˆ™å¿½ç•¥ï¼‰"""
    conn = sqlite3.connect(DB_PATH)
    conn.execute("""
        CREATE TABLE IF NOT EXISTS jobs (
            job_id TEXT PRIMARY KEY,
            title  TEXT,
            link   TEXT,
            seen_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    conn.commit()
    conn.close()

def upsert_and_get_new(conn: sqlite3.Connection, jobs: list[dict]) -> list[dict]:
    """å†™å…¥æœªè§è¿‡çš„èŒä½ï¼Œè¿”å›æœ¬è½®æ–°å¢çš„èŒä½åˆ—è¡¨"""
    new_jobs = []
    cur = conn.cursor()
    for j in jobs:
        job_id = j.get("job_id")
        if not job_id:
            continue
        cur.execute("SELECT 1 FROM jobs WHERE job_id = ?", (job_id,))
        if cur.fetchone():
            continue
        cur.execute(
            "INSERT INTO jobs(job_id, title, link) VALUES(?,?,?)",
            (job_id, j.get("title", ""), j.get("link", ""))
        )
        new_jobs.append(j)
    conn.commit()
    return new_jobs


# ========= æ ¸å¿ƒç›‘æ§æµç¨‹ =========
async def monitor_seek_jobs(playwright):
    context = await get_persistent_context(playwright)
    page = context.pages[0] if context.pages else await context.new_page()

    # å¯ä¿¡ä¼šè¯ä¸‹è®¿é—®æœç´¢é¡µ
    await page.goto(SEARCH_URL, wait_until="domcontentloaded")

    # è§¦å‘æ‡’åŠ è½½ï¼ˆæ»šåŠ¨å‡ æ¬¡ï¼‰
    for _ in range(6):
        await page.mouse.wheel(0, 1500)
        await page.wait_for_timeout(800)

    # è¿™é‡ŒåŠ å…¥â€œå¥å£®æ€§ä¸å¯è§‚æµ‹æ€§â€çš„ try/except
    try:
        jobs = await extract_seek_jobs(page)
    except Exception as e:
        html = await page.content()
        Path("debug_seek.html").write_text(html, encoding="utf-8")
        await page.screenshot(path="debug_seek.png", full_page=True)
        print("Extractor error:", e)
        # å¯é€‰ï¼šé€šçŸ¥ä¸€æ¬¡
        # notify_user(f"â—æŠ“å–å¤±è´¥ï¼š{e}")
        await context.close()
        return

    # 1) æŠ“å–èŒä½
    jobs = await extract_seek_jobs(page)

    # 2) å»é‡å…¥åº“ï¼Œè·å¾—â€œæ–°èŒä½â€
    conn = sqlite3.connect(DB_PATH)
    new_jobs = upsert_and_get_new(conn, jobs)
    conn.close()

    # 3) é€šçŸ¥
    if new_jobs:
        print(f"æœ¬è½®æŠ“å– {len(jobs)} æ¡ï¼Œæ–°å¢ {len(new_jobs)} æ¡ã€‚")
        for job in new_jobs:
            notify_user(f"ğŸ†• {job['title']} - {job['company']} ({job['location']})\nğŸ”— {job['link']}")
    else:
        print("No new jobs this cycle.")

    # æŒä¹…åŒ–ä¸Šä¸‹æ–‡ä¼šè‡ªåŠ¨ä¿å­˜ cookies
    await context.close()


# ========= å…¥å£ =========
async def main():
    init_db()
    while True:
        print(f"[{time.strftime('%H:%M:%S')}] å¼€å§‹æ£€æµ‹æ–°èŒä½...")
        async with async_playwright() as pw:
            await monitor_seek_jobs(pw)
        print(f"ç­‰å¾… {CHECK_INTERVAL // 60} åˆ†é’Ÿåå†æ¬¡æ£€æŸ¥...\n")
        time.sleep(CHECK_INTERVAL)

if __name__ == "__main__":
    asyncio.run(main())
